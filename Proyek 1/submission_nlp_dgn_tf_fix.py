# -*- coding: utf-8 -*-
"""Submission_NLP dgn_TF_fix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QhceEv7JrUNrEgNqiByQUnCKrsLTuCcA

# Proyek Pertama : Membuat Model NLP dengan TensorFlow

**NLP untuk analisis sentimen covid**


**Cecep Roni**

sumber dataset : https://www.kaggle.com/datatattle/covid-19-nlp-text-classification/download
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# load dataset
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Dicoding_PengembanganML/Submission/Coronatweets/Corona_NLP2.csv', encoding = 'ISO-8859-1')
df.head()

df.info()

df.shape

df.columns

# Hapus Kolom tak terpakai
df2 = df.drop(df.columns[[0,1,2,3]],axis=1)
df2

df2.rename(columns={'OriginalTweet': 'tweet', 'Sentiment' : 'sentiment'}, inplace=True)
df2

"""## Library and Package"""

# import library
import nltk, os, re, string

from keras.layers import Input, LSTM, Bidirectional, Dropout, SpatialDropout1D, Dense, Embedding, Flatten, BatchNormalization
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import Tokenizer, text_to_word_sequence
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
# from keras.utils import to_categorical

from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer

#download package
nltk.download('wordnet')
nltk.download('stopwords')

"""## Cleansing Data"""

def cleansing(data):
    #Lower Text
    data = data.lower()
    
    #hapus url
    data = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','',data)
    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    pattern1 = re.compile(r'pic.twitter.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    data = re.sub(pattern,' ',data) #remove urls if any
    data = re.sub(pattern1,' ',data)
    
    #Hapus Mention
    data = re.sub('@[^\s]+','',data)
    
    # hapus angka
    data = re.sub(r"\d+", "",data)
    
    # hapus covid
    data = re.sub(r"covid", "",data)
    
    #Hapus Hastag
    data = re.sub('#[^\s]+','',data)
    data = re.sub("b' ", "",data)
    data = re.sub("b'","",data)
    data = re.sub('b"','',data)

    #Remove additional white spaces
    data = re.sub('[\n]','',data)
    
    #Remove not alphanumeric symbols white spaces
    data = re.sub(r'[^\w]+', ' ', data)
    
    #Remove :( or :)
    data = data.replace(':)','')
    data = data.replace(':(','')
    
    #trim
    data = data.strip('\'"')
    
    return data

tweet_cleansing = []
for index, row in df2.iterrows():
    tweet_cleansing.append(cleansing(row["tweet"]))

df2['tweet'] = tweet_cleansing
df2.head()

df2.sentiment.hist()

df2.sentiment.value_counts()

# menyamaratakan jumlah kolom sentimen dengan mengambil jumlah yang terkecil
s_1 = df2[df2['sentiment']=='Extremely Negative'].sample(5481,replace=True)
s_2 = df2[df2['sentiment']=='Extremely Positive'].sample(5481,replace=True)
s_3 = df2[df2['sentiment']=='Negative'].sample(5481,replace=True)
s_4 = df2[df2['sentiment']=='Neutral'].sample(5481,replace=True)
s_5 = df2[df2['sentiment']=='Positive'].sample(5481,replace=True)
df3 = pd.concat([s_1, s_2, s_3, s_4, s_5])

print(df3.shape)
print(df3['sentiment'].value_counts(normalize=True))

df3.sentiment.hist()

"""## Model dan Plot"""

# data sentiment one-hot-encoding

sentiment = pd.get_dummies(df3.sentiment)
df_sentiment = pd.concat([df3, sentiment], axis=1)
df_sentiment = df_sentiment.drop(columns='sentiment')

df_sentiment.rename(columns={'Extremely Negative': 'extrm_negative', 'Extremely Positive' : 'extrm_positive', 'Negative' : 'negative',
                             'Neutral' : 'neutral', 'Positive' : 'positive'}, inplace=True)
df_sentiment.head()

# ubah dataframe value ke numpy array
tweets = df_sentiment['tweet'].values
label = df_sentiment[['extrm_negative', 'extrm_positive', 'negative', 'neutral', 'positive']].values

# tampilkan tweets dalam array
tweets

# tampilakn kolom sentimen dalam array
label

# split data ke train dan validation

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(tweets, label, test_size=0.2, shuffle=True)

#tokenisasi
#Import library

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Mulai Tokenisasi

tokenisasi = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenisasi.fit_on_texts(X_train) 
tokenisasi.fit_on_texts(X_val)
 
seq_train = tokenisasi.texts_to_sequences(X_train)
seq_test = tokenisasi.texts_to_sequences(X_val)
 
pad_train = pad_sequences(seq_train) 
pad_test = pad_sequences(seq_test)

# model
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)
model.summary()

# callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      self.model.stop_training = True
      print("\nThe accuracy of the training set and the validation set has reached > 90%!")
callbacks = myCallback()

# model fit
hist = model.fit(pad_train, y_train, epochs=50, 
                    validation_data=(pad_test, y_val), verbose=2, callbacks=[callbacks], validation_steps=30)

# plot akurasi

import matplotlib.pyplot as plt
plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# plot loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

